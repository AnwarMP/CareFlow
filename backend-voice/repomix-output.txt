This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-04-26T18:45:05.664Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
api/
  oncall.py
  postcall.py
app/
  agents.py
  config.py
  vector.py
requirements.txt
vercel.json

================================================================
Repository Files
================================================================

================
File: api/oncall.py
================
from fastapi import FastAPI
from pydantic import BaseModel
from app.agents import oncall_agent

app = FastAPI()

class Query(BaseModel):
    question: str

@app.post("/")
async def handle(query: Query):
    answer = oncall_agent(query.question)
    return {"answer": answer}

================
File: api/postcall.py
================
from fastapi import FastAPI, Header, HTTPException, Request
from app.agents import postcall_agent
from app.config import settings
from supabase import create_client
import hmac, hashlib, json

_cfg = settings()
_client = create_client(_cfg["SUPABASE_URL"], _cfg["SUPABASE_SERVICE_ROLE_KEY"])

SHARED_SECRET = os.environ.get("ELEVENLABS_WEBHOOK_SECRET")   # set in console

def verify(sig: str, body: bytes):
    mac = hmac.new(SHARED_SECRET.encode(), body, hashlib.sha256).hexdigest()
    if not hmac.compare_digest(mac, sig):
        raise HTTPException(401, "Invalid webhook signature")

app = FastAPI()

@app.post("/")
async def webhook(req: Request, x_elevenlabs_signature: str = Header(...)):
    raw = await req.body()
    verify(x_elevenlabs_signature, raw)
    payload = json.loads(raw)
    result  = postcall_agent(payload["transcript"])
    _client.table("call_logs").insert({
        "patient_id": payload["patientId"],
        "transcript": payload["transcript"],
        "summary": result["summary"],
    }).execute()
    return {"status": "ok"}

================
File: app/agents.py
================
from groq import Groq
from llama_index.core import PromptTemplate
from .vector import query_engine
from .config import settings

_cfg = settings()

_llm_oncall = Groq(api_key=_cfg["GROQ_API_KEY"], model="llama2-70b-7b-chat")  # low-latency chat :contentReference[oaicite:5]{index=5}
_llm_post   = Groq(api_key=_cfg["GROQ_API_KEY"], model="deepseek-r1-distill-llama-70b")  # heavyweight summariser :contentReference[oaicite:6]{index=6}

def oncall_agent(question: str) -> str:
    qe = query_engine()
    return str(qe.query(question, llm=_llm_oncall))

_SUMMARY_PROMPT = PromptTemplate(
    "Summarise the following medical call transcript. "
    "Extract (1) medication adherence issues, (2) pain level, "
    "(3) red-flag symptoms.  Transcript: {transcript}"
)

def postcall_agent(transcript: str) -> dict:
    prompt = _SUMMARY_PROMPT.format(transcript=transcript)
    summary = _llm_post.chat.completions.create(
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    ).choices[0].message.content
    return {"summary": summary}

================
File: app/config.py
================
import os, functools
from dotenv import load_dotenv
load_dotenv()                       # no-op on Vercel â€“ it injects env vars

@functools.lru_cache
def settings():
    return {
        "SUPABASE_URL":                 os.environ["SUPABASE_URL"],
        "SUPABASE_SERVICE_ROLE_KEY":    os.environ["SUPABASE_SERVICE_ROLE_KEY"],
        "GROQ_API_KEY":                 os.environ["GROQ_API_KEY"],
        "POSTGRES_CONN":                os.environ["POSTGRES_CONNECTION_STRING"],
        "EMBED_DIM":                    1536,              # keep in sync with model
    }

================
File: app/vector.py
================
from llama_index.vector_stores.supabase import SupabaseVectorStore
from llama_index.core import StorageContext, VectorStoreIndex
from supabase import create_client
from .config import settings

_cfg   = settings()
_client = create_client(_cfg["SUPABASE_URL"], _cfg["SUPABASE_SERVICE_ROLE_KEY"])

_vec_store = SupabaseVectorStore(
    postgres_connection_string=_cfg["POSTGRES_CONN"],
    collection_name="care_plans_collection",
    client=_client,
    table_name="care_plans",
    dimension=_cfg["EMBED_DIM"],
)

_storage_ctx = StorageContext.from_defaults(vector_store=_vec_store)
_index = VectorStoreIndex.from_vector_store(_storage_ctx)

def query_engine():
    # returns a cached QueryEngine instance
    return _index.as_query_engine()

================
File: requirements.txt
================
fastapi==0.111.0
uvicorn==0.29.0   # local testing
llama-index-core==0.10.35
llama-index-vector-stores-supabase==0.10.35
groq==0.7.0             # official SDK
supabase==2.5.3
python-dotenv==1.0.*    # optional local env loading

================
File: vercel.json
================
{
    "functions": {
      "api/*.py": {
        "runtime": "python3.11",
        "memory": 1024,
        "maxDuration": 15
      }
    }
  }
